# 测试理论

## 如果测试时间不够，你会怎么办？
##  如果让你去测试一个你完全不熟悉的系统，你会怎么办？
##  你平时会使用那些测试设计方法？
* 黑盒测试用例设计：等价类划分法、边界值分析法、错误推测法、因果图法、正交试验分析法、流程分析法
* 白盒测试：语句覆盖、判定覆盖、条件覆盖、条件组合覆盖、判定/条件覆盖、路径覆盖
##  介绍一下测试流程
* 需求评审、测试计划、测试用例、用例评审、冒烟测试、执行测试、追踪bug、问题跟进、测试报告、测试评估、上线\观察；
* 根据自己的日常经验来回答，每个点的工作内容都需要清晰掌握，有可能就某个点如何工作进行提问。
##  介绍一下软件测试方法
* 按阶段：单元测试、集成测试、系统测试、验收测试
* 按是否运行程序：静态测试、动态测试
* 按是否查看代码：黑盒测试、白盒测试、灰盒测试
* 黑盒测试之功能测试：界面测试、业务逻辑测试、兼容性测试、易用性测试、安装测试、反向测试
* 黑盒测试之性能测试：负载测试、压力测试、容量测试、并发测试、可靠性测试
* 其他：冒烟测试、回归测试、随机测试

## 黑盒测试
之所以被称为黑盒测试是因为可以将被测程序看成是一个无法打开的黑盒，而工作人员在不考虑任何程序内部结构和特性的条件下，根据需求规格说明书设计测试实例，并检查程序的功能是否能够按照规范说明准确无误的运行。其主要是对软件界面和软件功能进行测试。对于黑盒测试行为必须加以量化才能够有效的保证软件的质量。这种测试只关心输入和输出的结果，并不考虑程序的源代码

## 白盒测试
其与黑盒测试不同，它主要是借助程序内部的逻辑和相关信息，通过检测内部动作是否按照设计规格说明书的设定进行，检查每一条通路能否正常工作。白盒测试是从程序结构方面出发对测试用例进行设计。其主要用于检查各个逻辑结构是否合理，对应的模块独立路径是否正常以及内部结构是否有效。
* 常用的白盒测试法有控制流分析、数据流分析、路径分析、程序变异等，其中逻辑覆盖法是主要的测试方法。
* 逻辑覆盖标准：语句覆盖,分支覆盖,条件覆盖,路径覆盖,分支条件覆盖,覆盖率最高的是路径覆盖


## 灰盒测试
灰盒测试则介于黑盒测试和白盒测试之间。灰盒测试除了重视输出相对于出入的正确性，也看重其内部表现。但是它不可能像白盒测试那样详细和完整。它只是简单的靠一些象征性的现象或标志来判断其内部的运行情况，因此在内部结果出现错误，但输出结果正确的情况下可以采取灰盒测试方法。因为在此情况下灰盒比白盒高效，比黑盒适用性广的优势就凸显出来了
我个人理解其实，我们平常接触到的灰盒测试有：
* 检查数据库字段
* 检查redis标志字段
* 检查浏览器Local Storage/Cookies/Session Storage
* 接口测试

## 单元测试
逻辑覆盖，循环覆盖，同行评审，桌前检查，代码走查，代码评审，静态数据流分析

## Alpha测试
Alpha 测试是由一个用户在开发环境下进行的测试，也可以是公司内部的用户在模拟实际操作环境下进行的受控测试
* Alpha 测试不能由程序员或测试员完成
* Alpha 测试发现的错误，可以在测试现场立刻反馈给开发人员，由开发人员及时分析和处理
* Alpha 测试目的评价软件产品的功能、可使用性、可靠性、性能和支持。尤其注重产品的界面
和特色

## Beta测试
Beta 测试是软件的多个用户在一个或多个用户的实际使用环境下进行的测试
* 开发者通常不在测试现场，Beta测试不能由程序员或测试员完成
* Beta 测试是在开发者无法控制的环境下进行的软件现场应用
* Beta 测试着重于产品的支持性，包括文档、客户培训和支持产品的生产能力
* 只有当 Alpha 测试达到一定的可靠程度后，才能开始 Beta 测试

## 性能测试
* 参考 https://www.cnblogs.com/puresoul/p/5456855.html
* 软件性能：是软件的一种非功能特性，它关注的不是软件是否能够完成特定的功能，而是在完成该功能时展示出来的及时性
* 性能测试：指通过自动化的测试工具模拟多种正常、峰值以及异常负载条件来对系统的各项性能指标进行测试。

## 性能测试类型
* 基准测试：在给系统施加较低压力时，查看系统的运行状况并记录相关数做为基础参考
* 负载测试：是指对系统不断地增加压力或增加一定压力下的持续时间，直到系统的某项或多项性能指标达到安全临界值
* 压力测试：压力测试是评估系统处于或超过预期负载时系统的运行情况，关注点在于系统在峰值负载或超出最大载荷情况下的处理能力
* 稳定性测试：在给系统加载一定业务压力的情况下，使系统运行一段时间，以此检测系统是否稳定
* 并发测试：测试多个用户同时访问同一个应用、同一个模块或者数据记录时是否存在死锁或者其他性能问题

## 负载测试和压力测试的区别
* 负载测试是站在用户的角度去观察一定条件下软件系统的性能表现，压力测试可能要远远高于用户的性能需求
* 负载测试的预期结果是用户的性能需求得到满足，而压力测试的预期结果是系统出现问题
* 负载测试关心用户的规则和需求，压力测试关心软件系统本身的能力

## 性能测试的应用场景
| --- | 主要用途|典型场景|特点|测试方法|
|---|---|---|---|---|
| 能力验证 | 关注在给定的软硬件条件下，系统能否具有预期的能力表现|在要求平均响应时间小于2秒的前提下，如何判断系统是否能够支持50万用户/天的访问量？|a要求在已确定的环境下运行<br>b需要根据典型场景设计测试方案和用例，包括操作序列和并发用户量，需要明确的性能目标。|a负载测试<br>b压力测试<br>c稳定性测试|
|规划能力|关注如何使系统具有我们要求的性能能力|系统计划在一年内获客量在到xxx万，系统到时候是否能支持这么多用户量？如果不能需要如何调整系统的配置？|a它是一种探索性的测试<br>b常用于了解系统性能和获得扩展性能的方法|a负载测试<br>b压力测试<br>c配置测试|
|性能调优|主要用于对系统性能进行调优|系统上线运行一段时间后响应速度越来越慢，此时应该如何办？|每次只改变一个配置，切忌无 休止的调优|a并发测试<br>b压力测试<br>c配置测试|
|缺陷发现|发现缺陷或问题重现、定位手段|某些缺陷只有在高负载的情况下才能暴露出来，如线程锁、资源竞争或内存泄露。|做为系统测试的补充，用来发现并发问题，或是对系统已经出现的问题进行重现和定位|a并发测试<br>b压力测试|
|性能基准比较|常用于敏捷开发过程中，敏捷开发流程的特点是小步快走，快速试错，迭代周期短，需求变化频繁。很难定义完善的性能测试目标，也没有时间在每个迭代开展详细的性能测试，可以通过建立性能基线，通过比较每次迭代中的性能表现变化，判断迭代是否达到了目标。|---|---|---|

## 性能测试-响应时间
* 定义：从用户发送一个请求到用户接收到服务器返回的响应数据这段时间就是响应时间
* 计算方法：Response time = 网络传输时间 + 服务器（应用程序）处理时间
* 响应时间-负载对应关系：负载越大，响应时间基本不变，达到拐点后，响应时间急剧增大
a）响应时间突然增加<br>
b）意味着系统的一种或多种资源利用达到的极限<br>
c）通常可以利用拐点来进行性能测试分析与定位<br>

## 性能测试-吞吐量
* 定义：单位时间内系统处理的客户端请求的数量
* 计算单位：一般使用请求数／秒做为吞吐量的单位，出可以使用 页面数／秒表表示。从业务角度来说也可以使用 访问人数 /天 或 页面访问量／天 做为单位
* 计算方法：Throughput = (number of requests) / (total time)
* 资源利用-负载对应关系：负载越大，吞吐量逐渐增大，达到拐点后，急剧下降后达到饱和
a）吞吐量逐渐达到饱和<br>
b）意味着系统的一种或多种资源利用达到的极限<br>
c）通常可以利用拐点来进行性能测试分析与定位<br>

## 性能测试-资源利用率
* 定义：指的是对不同系统资源的使用程度，通常以占用最大值的百分比来衡量
* 服务器资源：CPU、内存、磁盘IO、网络宽带、数据库连接数、数据库读写时长、缓存吞吐量
* 计算方法：Throughput = (number of requests) / (total time)
* 资源利用-负载对应关系：负载越大，资源利用率逐渐增大，达到拐点后变成饱和不变
a）服务器某荐资源使用逐渐达到饱和<br>
b）通常可以利用拐点来进行性能测试分析与定位<br>

## 性能测试-其他概念
* 并发用户数：某一物理时刻同时向系统提交请求的用户数，提交的请求可能是同一个场景或功能，也可以是不同场景或功能。
* 最佳并发用户数：响应时间未到拐点，吞吐量和资源利用率刚达到（拐点）饱和时的并发用户数
* 最大并发用户数：响应时间刚到拐点，吞吐量下降，资源利用率饱和时的并发用户数
* 在线用户数：某段时间内访问系统的用户数，这些用户并不一定同时向系统提交请求
* 系统用户数：系统注册的总用户数据
* TPS：Transactions Per Second，每秒事务数
* 思考时间：用户每个操作后的暂停时间，或者叫操作之间的间隔时间，此时间内是不对服务器产生压力的
* 点击数：每秒钟用户向WEB服务器提交的HTTP请求数。这个指标是WEB应用特有的一个指标:WEB应用是"请求-响应"模式,用户发出一次申请,服务器就要处理一次，所以点击是WEB应用能够处理的交易的最小单位。如果把每次点击定义为一个交易，点击率和TPS就是一个概念。容易看出，点击率越大，对服务器的压力越大
* PV：访问一个URL，产生一个PV（Page View，页面访问量），每日每个网站的总PV量是形容一个 网站规模的重要指标
* UV：作为一个独立的用户，访问站点的所有页面均算作一个UV（Unique Visitor，用户访问）
* [理发店模型](https://www.cnblogs.com/jackei/archive/2006/11/20/565527.html)
* [地铁模型](https://www.cnblogs.com/puresoul/p/5458734.html)

## 你如何制定性能测试方案
* why 为什么要做性能测试？或者性能测试能解决业务上什么痛点？或者希望性能测试能发现什么用户使用上的问题？
* what 确定具体测试哪些业务、哪些数据库、哪些接口、程序架构等，整理出业务场景表
* where 确定测试的环境，是测试/仿真/线下环境？确定系统信息、硬件信息，服务器信息
* how 如何去做？经过成本的综合考虑，使用哪些测试工具（jmeter/LR/locust/OpenSTA）,选取什么性能测试方法（压力/负载/稳定性等）
* who 性能测试团队，包括DBA、系统开发人员（前后端）支持、性能测试设计分析人员（需求收集人员）、脚本开发/执行人员，有经验的负责人（各方面培训）
* when 一般而言，只有在系统基础功能测试验证完成，相对稳定的情况下，才会考虑性能测试，所以都准备好了，就可以预计性能测试各模块的起止时间

## 什么样才是符合性能需求

不能让用户明显感觉慢
* 页面长时间不动---响应时间长
* 系统用着用着就闪退/崩溃/卡顿----不稳定

所以，个人理解
* 页面跳转响应时间不能超过2秒
* 文件上传、下载不超过8秒
* 服务器cpu平均使用率小于80%
* 服务器内存平均使用率小于70%


## 如何判断是否内存泄漏

## 查看服务器性能的命令

## 什么是测试用例
测试用例是为某个特殊目标而编制的一组测试输入、执行条件以及预期结果，以便测试某个程序路径或核实是否满足某个特定需求
* 测试用例基本元素: 测试编号、测试标题、测试环境、测试前提条件、测试数据、测试步骤、预期结果、实际结果、测试结果、关联缺陷，关联需求、优先级

## 测试设计过程
* 根据需求文档、概要设计、详细设计整理测试子项
* 选择合适的测试用例管理工具--excel、TAPD
* 按照测试子项，根据测试用例设计方法书写测试用例
* 用例评审，完善/增加测试用例
* 维护用例，根据缺陷适当增加用例

## 测试用例设计方法
* 黑盒测试用例设计：等价类划分法、边界值分析法、错误推测法、因果图法、正交试验分析法、流程分析法
* 白盒测试：语句覆盖、判定覆盖、条件覆盖、条件组合覆盖、判定/条件覆盖、路径覆盖

## 设计一个登录页面的用例
* 首先查看登录页面功能使用需求文档，确定测试计划
* 功能测试：账号/密码/验证码/登录按钮分别测试、正确/空/特殊字符/长度/密码加密/大写提示
* 接口测试：功能测试类似，入参/响应结果测试
* 界面测试：布局/界面统一风格，错别字，多语言
* 兼容性测试：浏览器、系统、设备、软件版本
=======================================
* 易用性测试：Tab键切换/回车键、ctrl V/C、清空按钮、密码记住
* 性能测试：页面加载时间/页面跳转时间、接口并发请求、登录按钮连续快速点击
* 安全测试：脚本输入、多用户登录、多端登录、密码加密

## 举例项目推进的能力
* 推动文档质量：在日常工作中遇到需求文档、设计文档、接口文档不规范或不详细的在绝大多数，这个时候就要通过沟通或以bug的形式，促使各个岗位将各自的文档完善。
* 推动冒烟测试：冒烟不通过，测试召开会议罗列项目不通的模块、存在的问题，一一对应到每个人去跟进，得到解决的时间，后续项目群说明并艾特每个人跟进。测试准时验收。
* 结合自己的个人经验，从问题描述+处理过程+推进表现+结果，一一说明。

## 兼容性怎么测试
* 横向测试：软件运行环境考虑，如：iOS/Android,iphone/华为小米/三星/vivo/OPPO,谷歌/火狐/IE/Edge/safari
* 纵向测试：软件多版本考虑

## Web端测试和App端测试有何不同
* 系统构架方面：一个b/s,基于浏览器，web测试只要更新服务器端，客户端就好同步更新，一个c/s必须客户端，app修改了服务端，客户端需要升级，所以核心功能要做回归测试
* 兼容性测试不同：一个是浏览器兼容，一个是设备系统兼容
* app有专项测试，如安装、更新、卸载、干扰测试：中断/来电/关机等、界面操作：手势/横竖屏/触控/前后台切换,权限设置、存储空间、流量、电量


## 什么是埋点测试
* 埋点测试只是数据采集的一种术语，而数据采集是提供给运营工作人员去了解手机app对于某些模块、场景的用户使用情况.
* 进行的一个触发埋点，将埋点采集到的数据到的数据进行上报的过程。
* 采集数据只是起点，将数据进行分析、整理、汇总以及报表展示，最终得出用户对app普遍对使用行为，从而实现app面向用户的改良才是目的。
* 为了产品更好符合用户需求体验才是终点。

## 印象深刻的一个bug？

## 测试中遇到的比较难的一个项目是？

## 如何设计好的测试用例,提高测试覆盖率?提高测试效率?

## 如何展现测试成果?

## 如何编写专业化、优质的测试文档？

## 如何适时借助测试工具，测试一些重复性比较高的测试？

## 如何引入自动化测试？

## 如何合理安排好每个测试任务？调解好每件与测试相关的情况?

## 如何合理做好分工？做好每个下属的绩效考核

## 日常工作中 JMeter 是怎么用的？
* 接口测试：通过对指定接口进行请求访问，验证数据出入的准确性与安全性；
* 制造测试数据, 100条站内信测试临界点，CVS数据制造

## 例举熟悉的自动化测试工具，并说明其实现原理

## 怎么提高selenium脚本的自动化执行效率？
* 时间：尽量少用sleep, 转为使用selenium提供的WebDriverWait().until
* 规避环境因素：少用IE/确保网络顺畅
* 前提条件：比较登录态获取问题，是否可以提前准备而不需要从页面登录开始测试，然后直接使用url进入待测试页面
* 重试机制